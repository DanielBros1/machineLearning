{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3. Get the nouns in the last 10 sentences from Trump's speech and find the nouns divided by sentencens. Use SpaCy.\n",
        "\n",
        "Please use Python list features to get the last 10 sentences and display nouns from it."
      ],
      "metadata": {
        "id": "Y9jZP2Ll68Kb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "file = open(\"trump.txt\", \"r\",encoding='utf-8')\n",
        "trump = file.read()\n",
        "\n",
        "### here comes your code\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "document = nlp(trump)\n",
        "sentences = list(document.sents)[-10:]\n",
        "nouns = []\n",
        "for sentence in sentences:\n",
        "    nouns.append([token.text for token in sentence if token.pos_ == \"NOUN\"])\n",
        "print(nouns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoEogq-U1h7i",
        "outputId": "0431e1e5-14a4-4a7a-b1bd-0c49c6a8f7a9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['vision', 'years', 'freedom', 'tonight', 'chapter', 'greatness'], ['time', 'thinking'], ['time', 'fights'], ['courage', 'dreams', 'hearts', 'bravery', 'hopes', 'souls', 'confidence', 'hopes', 'dreams', 'action'], ['aspirations', 'fears', 'future', 'failures', 'past', 'vision', 'doubts'], ['citizens', 'renewal', 'spirit'], ['Members', 'things', 'country'], ['tonight', 'moment'], ['yourselves', 'future'], []]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 4. Build your own Bag Of Words implementation using tokenizer created before\n",
        "\n",
        "You need to implement following methods:\n",
        "\n",
        "- ``fit_transform`` - gets a list of strings and returns matrix with it's BoW representation\n",
        "- ``get_features_names`` - returns list of words corresponding to columns in BoW\n"
      ],
      "metadata": {
        "id": "GtFaBXS07DiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import spacy\n",
        "\n",
        "class BagOfWords:\n",
        "    \"\"\"Basic BoW implementation.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.__nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
        "        self.__feature_names = []\n",
        "\n",
        "    def __tokenize(self, text: str) -> list:\n",
        "        doc = self.__nlp(text.lower())\n",
        "        return [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
        "\n",
        "    def fit_transform(self, corpus: list):\n",
        "\n",
        "        tokenized_docs = [self.__tokenize(doc) for doc in corpus]\n",
        "\n",
        "        vocabulary = sorted(set(token for doc in tokenized_docs for token in doc))\n",
        "        self.__feature_names = vocabulary\n",
        "\n",
        "        matrix = np.zeros((len(corpus), len(vocabulary)), dtype=int)\n",
        "\n",
        "        for i, tokens in enumerate(tokenized_docs):\n",
        "            for token in tokens:\n",
        "                j = vocabulary.index(token)\n",
        "                matrix[i][j] += 1\n",
        "\n",
        "        return matrix\n",
        "\n",
        "\n",
        "    def get_feature_names(self) -> list:\n",
        "        \"\"\"Return words corresponding to columns of matrix.\n",
        "        \"\"\"\n",
        "        return self.__feature_names\n",
        "\n",
        "corpus = [\n",
        "     'Bag Of Words is based on counting',\n",
        "     'words occurences throughout multiple documents.',\n",
        "     'This is the third document.',\n",
        "     'As you can see most of the words occur only once.',\n",
        "     'This gives us a pretty sparse matrix, see below. Really, see below',\n",
        "]\n",
        "\n",
        "vectorizer = BagOfWords()\n",
        "\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(X)\n",
        "\n",
        "vectorizer.get_feature_names()\n",
        "len(vectorizer.get_feature_names())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evukKD--5C2c",
        "outputId": "ac4c6ebc-f165-4467-96fb-0aa931b4cbd6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 1 1 0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 1 0 0 1 0 1 0 0 1]\n",
            " [0 0 0 1 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 1 0 0 0 1]\n",
            " [0 0 0 0 1 1 0 0 0 1 1 0]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    }
  ]
}